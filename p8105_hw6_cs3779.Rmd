---
title: "p8105_hw6_cs3779"
author: "CJ Snyder"
date: "11/19/2019"
output: github_document
---

```{r, include = FALSE}
library(tidyverse)
library(knitr)
library(readxl)
library(viridis)
library(modelr)
library(mgcv)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# **Problem 1**
## **Data Tidying**
```{r}
bw_df = 
  read_csv("./data/birthweight.csv",
           col_types = "fdddddfdfdddfddddddd") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex, levels=c(1,2), labels=c("male", "female")),
    frace = factor(frace, levels=c(1,2,3,4,8,9), labels = c("white", "black", "asian", "puerto rican", "other", "unknown")),
    malform = factor(malform, levels=c(0,1), labels=c("absent", "present")),
    mrace = factor(mrace, levels=c(1,2,3,4,8), labels = c("white", "black", "asian", "puerto rican", "other")),
    ppbmi = round(ppbmi, digits = 2),
    delbmi = round(703 * delwt / (mheight)^2, digits = 2)
  )

colSums(is.na(bw_df))
```

Notes: No missing values for any of the columns.

## **My Model Building**
```{r}
linearmod = lm(bwt ~ babysex + delbmi + gaweeks + fincome + momage + ppbmi + smoken, data = bw_df)

linearmod %>% 
  broom::tidy() %>% 
  kable(digits = 3)

linearmod_frace = lm(bwt ~ babysex + delbmi + gaweeks + fincome + momage + ppbmi + smoken + frace, data = bw_df)

linearmod_mrace = lm(bwt ~ babysex + delbmi + gaweeks + fincome + momage + ppbmi + smoken + mrace, data = bw_df)

anova(linearmod, linearmod_frace) %>% 
  broom::tidy()

anova(linearmod, linearmod_mrace) %>% 
  broom::tidy()

linearmod_frace %>% 
  broom::tidy() %>% 
  kable(digits = 3)

linearmod_final = lm(bwt ~ babysex + delbmi + gaweeks + ppbmi + smoken + frace, data = bw_df)

linearmod_final %>% 
  broom::tidy() %>% 
  mutate(term = str_replace(term, "^frace", "Father's Race: ")) %>% 
  kable(digits = 3)
```

Notes: I started with covariates that wouldn't be too colinear with birthweight like birth length or head circumfrance (i.e. where a higher value in one automatically determines a higher value in the other without a clear causal direction). I then went through each variable adding it to the model to test if that variable and the other variables were/remained significant. If so, I kept them in the model. I conducted partial F-tests to determine if categorical variables were significant. _frace_ was significant compared to _mrace_, but after including it in the model, _momage_ and _fincome_ became insignificant, so I removed them from the final model. I tried to be as parsimonious as well by including BMIs instead of both height and weight (pre-pregnancy and delivery).

## **My Model Diagnostics**
```{r}
resid = modelr::add_residuals(bw_df, linearmod_final) %>% 
  select(resid)
pred = modelr::add_predictions(bw_df, linearmod_final) %>% 
  select(pred)

resid_pred = 
  bind_cols(resid, pred) %>% 
  ggplot(aes(x=resid, y=pred)) + geom_point()

resid_pred_vio = 
  bind_cols(resid, pred) %>% 
  ggplot(aes(x=resid, y=pred)) + geom_violin()

resid_pred
resid_pred_vio
```

## **Provided Model Building**
```{r}
linmod_1 = lm(bwt ~ blength + gaweeks, data = bw_df)

linmod_1 %>% 
  broom::tidy() %>% 
  kable(digits = 3)

linmod_2 = lm(bwt ~ babysex + blength + bhead + babysex*blength + babysex*bhead + blength*bhead + babysex*blength*bhead, data = bw_df)

linmod_2 %>% 
  broom::tidy() %>% 
  kable(digist = 3)
```

## **Cross-Validation between My and Provided Models**
```{r}
cv_df = 
  crossv_mc(bw_df, 100)

cv_df =  
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>% 
  mutate(
    linearmod_final = map(train, ~lm(bwt ~ babysex + delbmi + gaweeks + ppbmi + smoken + frace, data = .x)),
    linmod_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    linmod_2 = map(train, ~lm(bwt ~ babysex + blength + bhead + babysex*blength + babysex*bhead + blength*bhead + babysex*blength*bhead, data = .x))
  ) %>% 
  mutate(
    rmse_linmod_final = map2_dbl(linearmod_final, test, ~rmse(model = .x, data = .y)),
    rmse_linmod_1 = map2_dbl(linmod_1, test, ~rmse(model = .x, data = .y)),
    rmse_linmod_2 = map2_dbl(linmod_2, test, ~rmse(model = .x, data = .y))
  )
```

### **Comparing My Model to 1st Provided Model**
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
    ) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x=model, y=rmse)) + geom_violin()
```

Note: Based on the rmse values, it would seem that the provided models (estimating birth weight by baby length and gestational weeks, and estimating birth weight by baby length, gestational weeks, head circumfrance, sex, and all of their interactions) are better than the built model since they both have lower rmse values than the built model.

## **Problem 2**
```{r, echo=FALSE, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

set.seed(1)
```

### **Creating Boot Strap Samples**
```{r}
boot_sample = function(x) {
  sample_frac(x, replace = TRUE)
}

bootsamp_df = 
  tibble(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

bootsamp_results_betas = 
  bootsamp_df %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy) 
  ) %>% 
  select(-models, -strap_sample) %>% 
  unnest()
```

### **Analyzing Betas**
```{r}
bootsamp_betas_int = 
  bootsamp_results_betas %>% 
  select(strap_number, term, estimate) %>% 
  filter(term=="(Intercept)")

bootsamp_betas_x =
  bootsamp_results_betas %>% 
  select(strap_number, term, estimate) %>% 
  filter(term=="tmin")

bootsamp_betas = 
  inner_join(bootsamp_betas_int, bootsamp_betas_x, by="strap_number") %>% 
  mutate(
    logbetas = log(estimate.x*estimate.y)
  )
```

### **Analyzing r-squares**
```{r}
bootsamp_results_rsq = 
  bootsamp_df %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance) 
  ) %>% 
  select(-models, -strap_sample) %>% 
  unnest() %>% 
  select(strap_number, r.squared)

bootsamp_final = 
  inner_join(bootsamp_betas, bootsamp_results_rsq, by="strap_number") %>% 
  select(strap_number, logbetas, r.squared)

ggplot(bootsamp_final, aes(x=logbetas, y=r.squared)) + geom_point()
```

Note: From looking at the distribution of R-squared values compared to the Log(Beta_0 * Beta_1) value of each bootstrapped sample, it seems that as the value of logbetas grew, the r-square value drops (i.e. a negative correlation between the two values).


### **95% Confidence Intervals for Betas and R-squares**
```{r}
kable(quantile(bootsamp_final$logbetas, c(0.025, 0.975)))
kable(quantile(bootsamp_final$r.squared, c(0.025, 0.975)))
```



